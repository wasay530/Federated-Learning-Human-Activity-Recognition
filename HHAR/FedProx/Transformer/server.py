import flwr as fl
import numpy as np
import random
import statistics
import tensorflow as tf
import numpy as np
import os
os.environ["CUDA_VISIBLE_DEVICES"] = "-1"

# Number of independent experiments
num_experiments = 30
accuracy_scores = []

# Function to set a new random seed for each independent run
def set_random_seed(seed):
    random.seed(seed)
    np.random.seed(seed)

# Positional Encoding Layer
class PositionalEncoding(tf.keras.layers.Layer):
    def __init__(self, maxlen, embed_dim, **kwargs):
        super(PositionalEncoding, self).__init__(**kwargs)
        self.maxlen = maxlen
        self.embed_dim = embed_dim
        self.positional_encoding = self.get_positional_encoding(maxlen, embed_dim)

    def get_positional_encoding(self, maxlen, embed_dim):
        pos = tf.range(maxlen, dtype=tf.float32)[:, tf.newaxis]
        i = tf.range(embed_dim, dtype=tf.float32)[tf.newaxis, :]
        angle_rates = 1 / tf.pow(10000.0, (2 * (i // 2)) / embed_dim)
        angles = pos * angle_rates
        pos_enc = tf.where(tf.cast(i, tf.int32) % 2 == 0, tf.math.sin(angles), tf.math.cos(angles))
        return tf.cast(pos_enc, tf.float32)

    def call(self, inputs):
        return inputs + self.positional_encoding[: tf.shape(inputs)[1], :]

    def get_config(self):
        config = super(PositionalEncoding, self).get_config()  # Correct super() call
        config.update({
            "maxlen": self.maxlen,
            "embed_dim": self.embed_dim,
        })
        return config


# Transformer Block Layer
class TransformerBlock(tf.keras.layers.Layer):
    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):
        super(TransformerBlock, self).__init__(**kwargs)
        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)
        self.ffn = tf.keras.Sequential([
            tf.keras.layers.Dense(ff_dim, activation="relu"),
            tf.keras.layers.Dense(embed_dim),
        ])
        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)
        self.dropout1 = tf.keras.layers.Dropout(rate)
        self.dropout2 = tf.keras.layers.Dropout(rate)

    def call(self, inputs, training=None):
        attn_output = self.att(inputs, inputs)
        attn_output = self.dropout1(attn_output, training=training)
        out1 = self.layernorm1(inputs + attn_output)
        ffn_output = self.ffn(out1)
        ffn_output = self.dropout2(ffn_output, training=training)
        return self.layernorm2(out1 + ffn_output)


# Create the Transformer Model outside of the CifarClient class
def create_transformer_model(input_shape, embed_dim=128, num_heads=4, ff_dim=256, maxlen=200):
    inputs = tf.keras.layers.Input(shape=input_shape)
    x = tf.keras.layers.Dense(embed_dim)(inputs)
    x = PositionalEncoding(maxlen, embed_dim)(x)
    
    # Adding transformer blocks
    x = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(x)
    x = TransformerBlock(embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim)(x)

    # Global Average Pooling and Dense Layers for classification
    x = tf.keras.layers.GlobalAveragePooling1D()(x)
    x = tf.keras.layers.Dense(64, activation="relu")(x)
    x = tf.keras.layers.Dropout(0.3)(x)
    x = tf.keras.layers.Dense(32, activation="relu")(x)
    outputs = tf.keras.layers.Dense(10, activation="softmax")(x)  # Example output for 10 classes
    
    model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
    
    # Compile the model with Adam optimizer
    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.005),
                  loss=tf.keras.losses.SparseCategoricalCrossentropy(),
                  metrics=["accuracy"])
    return model

# Create the transformer model
model = create_transformer_model(input_shape=(30, 3))

initial_parameters = fl.common.ndarrays_to_parameters([np.array(w) for w in model.get_weights()])

# Run FL training 30 times
for i in range(num_experiments):
    print(f"\nðŸ”¥ Running Independent FL Experiment {i+1}/{num_experiments} with Random Seed {i}")
    set_random_seed(i)  # Set a different seed for each experiment

    # Define the FedAvg strategy
    strategy = fl.server.strategy.FedProx(
        proximal_mu=0.1,  # Proximal regularization parameter
        min_fit_clients=9,
        min_available_clients=9,
        initial_parameters=initial_parameters,
    )

    # Start the Flower server
    fl.server.start_server(
        server_address="127.0.0.1:8091",
        config=fl.server.ServerConfig(num_rounds=200),
        strategy=strategy
    )

    # Simulate model evaluation (Assuming client sends back accuracy)
    final_accuracy = random.uniform(0.7, 0.9)  # Placeholder for actual accuracy received from clients
    accuracy_scores.append(final_accuracy)

# Compute mean and standard deviation across 30 runs
mean_acc = statistics.mean(accuracy_scores)
std_acc = statistics.stdev(accuracy_scores)

print(f"\nâœ… Final FL Results after {num_experiments} Independent Runs:")
print(f"Mean Accuracy: {mean_acc:.4f}, Standard Deviation: {std_acc:.4f}")

# Save results to a file (optional)
import pandas as pd

results_df = pd.DataFrame({
    "Experiment": list(range(1, num_experiments + 1)),
    "Accuracy": accuracy_scores
})

results_df.loc["Mean"] = ["-", mean_acc]
results_df.loc["Std Dev"] = ["-", std_acc]

results_df.to_csv("fl_server_results.csv", index=False)
print("Results saved to fl_server_results.csv")

